---
title: "Model Output"
date: "November 12, 2014"
output: 
    html_document:
        number_sections: true
        css: basic.css
        self_contained: false
        theme: null
        highlight: null
        keep_md: true
---

```{r initialize, eval=T, echo=F, collapse=T, results='hide'}
# setwd('~/win/project/ellis-nsf10/STEM/analysis')
setwd('~/Documents/Geog/MigProject/STEM/analysis')
require(sandwich)
require(msm)
require(knitr)
require(magrittr)

require(RSQLite)

source("finalfinalmakedata.R")
source("~/Documents/rfuns/plotfuns.R")
# Some final data mods
# And make a reduced dataset with just stem degree holders
a = acs[acs$stemdeg == 1,]
a = transform(a, 
    agebin = cut(age, 
    breaks = c(18, 24, 29, 34, 39, 44, 49, 54, 59, 64, 74, 84, 95)))

voi = c("perwt", "age", "fb", "geog_curr", "geogname", "stemjob", "stem1", "stem_domain", "ed", "stemdeg", "match", "f_race", "f_sex", "f_fb", "agebin", "stemjoblq", "stemjobcount")

# write.csv(acs, file = "../data/acs.csv", row.names = F)
# write.csv(acs.stemdeg, file = "../data/acs.stemdeg.csv", row.names = F)

a = with(a, a[age >= 25 & age <= 65, voi])
a = transform(a, agec = round(age - mean(age), 1))
a = transform(a, stemjoblqc = stemjoblq - 1)
a = transform(a, stemjobcountc = round(stemjobcount - mean(stemjobcount)))

# and my cute plotting functions
source("plotfuns.R")

# And this stupidity...
citynames = c("Atlanta, GA", "Austin, TX", "Baltimore, MD", "Boston, MA", "Buffalo, NY", "Charlotte, NC", "Chicago, IL", "Cincinnati, OH", "Cleveland, OH", "Columbus, OH", "Dallas-Fort Worth, TX", "Denver, CO", "Detroit, MI", "Fort Lauderdale, FL", "Fresno, CA", "Grand Rapids, MI", "Greensboro, NC", "Houston, TX", "Indianapolis, IN", "Jacksonville, FL", "Kansas City, MO", "Las Vegas, NV", "Los Angeles, CA", "Memphis, TN", "Miami, FL", "Milwaukee, WI", "Minneapolis-St. Paul, MN", "Monmouth-Ocean, NJ", "Nashville, TN", "New Orleans, LA", "New York, NY", "Norfolk, VA", "Oklahoma City, OK", "Orlando, FL", "Philadelphia, PA", "Phoenix, AZ", "Pittsburgh, PA", "Portland, OR", "Providence, RI", "Raleigh-Durham, NC", "Richmond, VA", "Riverside, CA", "Rochester, NY", "Sacramento, CA", "Salt Lake City, UT", "San Antonio, TX", "San Diego, CA", "San Francisco, CA", "San Jose, CA", "Seattle, WA", "St. Louis, MO", "Tampa-St. Petersburg, FL", "Tulsa, OK", "Washington, DC", "West Palm Beach, FL")

# Adrian Raftery's BIC.lm function
BIC.lm.a = function (lm.object) {
    # Computes BIC for a fixed effects regression model estimated using lm.
    # The definition of BIC is the same as that used in lmer.
    sigma2 = mean (lm.object$residuals^2)
    df = summary(lm.object)$df
    n = df[1] + df[2]
    npar = df[1] + 1
    
    loglik = (-n/2) * (log(2*pi) + log(sigma2) + 1)
    BIC = -2*loglik + npar*log(n)
    return(BIC)
}

# A function for calculating the log-likelihood used in AR's function
ll.a = function(mod) {
    sigma2 = mean (mod$residuals^2)
    df = summary(mod)$df
    n = df[1] + df[2]
    loglik = (-n/2) * (log(2*pi) + log(sigma2) + 1)
    df = df[3]
    npar = df[1] + 1

    return(list(log.likelihood = loglik, parameters = npar, df = df, n = n))
}

# Uses the built-in method for extracting logLikelihood
# results correspond more closely with LR tests, etc.
BIC.lm = function(mod) {
    loglik = as.numeric(logLik(mod))
    df = summary(mod)$df
    npar = df[1] + 1
    n = df[1] + df[2]
    
    BIC = -2 * loglik + npar * log(n)
    return(BIC)
}

```

The big difference is that we lose almost *half* of the observations once we eliminate those with advanced degrees. By classifying second degree fields, we picked up about 1,000 additional STEM degree holders to bring the number to about 92,000. But by eliminating those with an advanced degree, we lost about 85,000. 

That is astounding.

# Model

Main effects, then interact sex, nativity, race. The intercept is a white male of average age (about 42.8 yrs) in a city with a STEM job LQ of 1.

```{r modprep, echo=FALSE, eval=TRUE, results='hide'}
require(msm)

# all of the following should have been taken care of in 
# makedata.R
# a = transform(a, stemjoblqc = stemjoblq - mean(stemjoblq))
# a[,"f_race"] = relevel(a[,"f_race"], ref = "W")
a[,"f_sex"] = relevel(a[,"f_sex"], ref = "M")
a[,"f_fb"] = relevel(a[,"f_fb"], ref = "nb")

kable(data.frame(meanLQ = round(mean(a$stemjoblq), 2), mean.age = round(mean(a$age), 1)))
```

## LQ or log(count)?

```{r lqorcount.mod, echo=FALSE, eval=TRUE, results='hide'}

# mod.1 = glm(match ~ stemjoblq + f_race + f_sex + f_fb + agec + I(agec^2), data = a, family = quasibinomial(link = "log"), start = rep(0.5, 9))
# if the above does not work...
modbase = glm(match ~ f_race + f_sex + f_fb + agec + I(agec^2), data = a, family = poisson)
# mod.logit = glm(match ~ stemjoblqc + log(stemjobcount) + f_race + f_sex + f_fb + agec + I(agec^2), data = a, family = binomial(link = "logit"))

summary(modbase)
# Correlation between fitted and actual values
# Which will always be poor since the outcome is 
# The relative risk of match == 1
cor(exp(modbase$fitted), a$match)^2

# remove a factor at a time and test. How the fuck to report this...
modcount = update(modbase, . ~ . + stemjobcountc)
modlq = update(modbase, . ~ . + stemjoblqc)
modboth = update(modbase, . ~ . + stemjobcountc + stemjoblqc)
# modboth = glm(match ~ f_race + f_sex + f_fb + agec + I(agec^2) + stemjoblqc + log(stemjobcount), data = a, family = poisson)

modbasesum = summary(modbase)
modcountsum = summary(modcount)
modlqsum = summary(modlq)
modbothsum = summary(modboth)

1-pchisq(modbasesum$deviance, modbasesum$df.residual) # if this is sigificant we have a bad fit
with(modbase, pchisq(deviance, df.residual, lower.tail = F)) # if this is sig, we have a poor fit
with(modcount, pchisq(deviance, df.residual, lower.tail = F)) # if this is sig, we have a poor fit
with(modlq, pchisq(deviance, df.residual, lower.tail = F)) # if this is sig, we have a poor fit
with(modboth, pchisq(deviance, df.residual, lower.tail = F)) # if this is sig, we have a poor fit


summary(modcount)
summary(modlq)

anova(modbase, modcount, test = "Chisq")
anova(modbase, modlq, test = "Chisq")
anova(modbase, modboth, test = "Chisq")

anova(modboth, modcount, modbase, test = "Chisq")
anova(modboth, modlq, modbase, test = "Chisq")

ll.1 = as.numeric(logLik(modbase))
ll.2 = as.numeric(logLik(modcount))
ll.3 = as.numeric(logLik(modlq))
ll.4 = as.numeric(logLik(modboth))

# Same as ANOVA
pchisq(2 * (ll.2 - ll.1), df = (summary(modcount)$df[1] - summary(modbase)$df[1]), lower.tail = FALSE)
pchisq(2 * (ll.3 - ll.1), df = (summary(modlq)$df[1] - summary(modbase)$df[1]), lower.tail = FALSE)

# We have two ways of computing the BIC
# The function adds 1 to the number of parameters
BIC.lm(modbase) ; -2*(ll.1) + log(nrow(a)) * summary(modbase)$df[1]
BIC.lm(modcount) ; -2*(ll.1) + log(nrow(a)) * summary(modcount)$df[1]
BIC.lm(modlq) ; -2*(ll.1) + log(nrow(a)) * summary(modlq)$df[1]

BIC.lm(modcount) ; BIC.lm(modbase) ; BIC.lm(modcount) - BIC.lm(modbase)
BIC.lm(modlq) ; BIC.lm(modbase) ; BIC.lm(modlq) - BIC.lm(modbase)


# LR test
lr.test <- 2*(ll.2 - ll.1)
lr.test.p <- pchisq(lr.test,df=(summary(modcount)$df[1] - summary(modbase)$df[1]),lower.tail=FALSE)

# BIC
bic.test.2 <- -2*(ll.2 - ll.1) + log(nrow(a)) * (summary(modbase)$df[1] + 1)
bic.test.3 <- -2*(ll.3 - ll.1) + log(nrow(a)) * (summary(modbase)$df[1] + 1)
bic.test.2
bic.test.3
```

We have four models, one with neither stem job count nor the LQ, then three with all possible combinations.

```{r lqorcount.pres, echo=FALSE, eval=TRUE, results='asis'}
names = names(coef(modboth))

results = data.frame(matrix(data = NA, nrow = length(names), ncol = 4))
rownames(results) = names
colnames(results) = c("base", "count", "lq", "both")

c.modbase = coef(modbase) %>% exp
n.modbase = names(c.modbase)

c.modcount = coef(modcount) %>% exp 
n.modcount = names(c.modcount)

c.modlq = coef(modlq) %>% exp 
n.modlq = names(c.modlq)

c.modboth = coef(modboth) %>% exp 
n.modboth = names(c.modboth)

indbase = match(n.modbase, n.modboth) 
indcount = match(n.modcount, n.modboth)
indlq = match(n.modlq, n.modboth)

results[indbase, "base"] = c.modbase
results[indcount, "count"] = c.modcount
results[indlq, "lq"] = c.modlq
results[,"both"] = c.modboth

# results = round(results, 3)

bic1 = round(BIC.lm(modbase), 0)
bic2 = round(BIC.lm(modcount), 0)
bic3 = round(BIC.lm(modlq), 0)
bic4 = round(BIC.lm(modboth), 0)

ll.1 = round(as.numeric(logLik(modbase)), 0)
ll.2 = round(as.numeric(logLik(modcount)), 0)
ll.3 = round(as.numeric(logLik(modlq)), 0)
ll.4 = round(as.numeric(logLik(modboth)), 0)

ll.count = -2 * (logLik(modbase) - logLik(modcount))[1]
df.count = summary(modcount)$df[1] - summary(modbase)$df[1]
pchisq(ll.count, df = df.count, lower.tail = F)
lrtest.count = pchisq(ll.count, df = df.count, lower.tail = F)
anova(modbase, modcount, test = "Chisq")

lrtest.count = anova(modbase, modcount, test = "Chisq")[["Pr(>Chi)"]][2]
lrtest.lq = anova(modbase, modlq, test = "Chisq")[["Pr(>Chi)"]][2]
lrtest.both = anova(modbase, modboth, test = "Chisq")[["Pr(>Chi)"]][2]

loglikrow = c(ll.1, ll.2, ll.3, ll.4)
lrtestrow = c(NA, lrtest.count, lrtest.lq, lrtest.both)
bicrow = c(bic1, bic2, bic3, bic4)

resultsout = data.frame(rbind(results, loglikrow, lrtestrow, bicrow))
rownames(resultsout)[11:13] = c("LogLikelihood", "Pr[>Chi]", "BIC")
round(resultsout, 3)

kable(resultsout, digits = 3)
```

This result is different from what we got before in which the centered LQ did all the work. The evidence is mixed. By itself, the stem job count does not improve model fit. But in the model with LQ, we have evidence that it does. Furthermore they pull in opposite directions.

This is a pretty big change from when we ran the models with STEM degree holders with either a BA or an advanced degree. Why?


```{r advdegattainment, echo=FALSE, eval=TRUE, collapse=TRUE, results='asis'}
sqlite = dbDriver("SQLite")
dbpath = "~/Documents/Geog/MigProject/data/sql/acs2011.sqlite"

conn = dbConnect(sqlite, dbpath)

query = "SELECT    
		acs.perwt,
		acs.age,
		acs.sex,
		acs.citizen,
		acs.fb,
		acs.degfield,
		acs.degfieldd,
		acs.empstat,
		acs.ed,
		acs.ethrace,
		acs.geog_prev,
		acs.geog_curr,
        acs.stemdeg,
		codematch.ipums_code,
		codematch.stem as stemjob,
		codematch.stem_domain,
		codematch.stem1,
		codematch.stem4,
		degreclass.degreclass as degreclass,
		reclasscodes.reclasslabel as reclasslabel,
        citymapper.code,
        citymapper.geogname
	from acs 
	LEFT JOIN codematch ON acs.occsoc = codematch.ipums_code
	LEFT JOIN degreclass ON acs.degfield = degreclass.degfield
	LEFT JOIN reclasscodes ON degreclass.degreclass = reclasscodes.reclasscode
	LEFT JOIN citymapper on acs.geog_curr = citymapper.code
	where 
		acs.civ_hh = 1
        AND	(acs.gq != 3 and acs.gq !=4) 
        AND citymapper.pop2010 > 1000000
		AND acs.geog_curr > 0
        AND acs.ethrace < 4
        AND (acs.empstat = 1 or acs.empstat = 2);"

########################################################
start.time  =  proc.time()

q.obj = dbSendQuery(conn, query)

# Retrieve Query results into a data frame
acs.all = fetch(q.obj, n=-1)

print(paste("time in minutes: ", round(proc.time()[3] - start.time[3], 3) / 60, sep=""))
########################################################

# Clean up the query object (no longer needed)
dbClearResult(q.obj)

# Indicator for STEM degree/job match
acs.all = transform(acs.all, match = as.numeric(stem1 & stemdeg))

acs.all %>% with(table(ed, stemdeg)) %>% kable(digits = 0, row.names = T, format = "pandoc", caption = "Attainment X STEM Degree: all LF")

attainmentXSTEM = acs.all[acs.all$age >= 25 & acs.all$age <= 65,] %>% with(table(ed, stemdeg)) 
attainmentXSTEM %>% kable(digits = 0, row.names = T, format = "pandoc", caption = "Attainment X STEM Degree: all LF aged 25 - 65")
```

The rows correspond to highest educational attainment. 4 is a BA, and 5 is an advanced degree. When we exclude advanced degree holders, we will lose almost half the STEM degree holders.

```{r stemdegprop, echo=FALSE, eval=TRUE, collapse=TRUE, results='asis'}
attainmentXSTEMct = acs.all[acs.all$age >= 25 & acs.all$age <= 65,] %>% with(tapply(perwt, list(ed, stemdeg), sum)) 

stemdegholders = apply(attainmentXSTEMct[4:5,1:2], 2, sum)[2]
alldegholders = sum(attainmentXSTEMct[4:5,1:2])
stemdegholders/ alldegholders
```

That is just because advanced degree attainment among STEM degree holders is astounding:

```{r advdegattainmentprop, echo=FALSE, eval=TRUE, collapse=TRUE, results='asis'}
advprop = attainmentXSTEM[5,] / apply(attainmentXSTEM[4:5,], 2, sum) 
advprop %>% kable(digits = 2, format = "pandoc", caption = "Advanced degree attainment, non-STEM and STEM degree holders (observations).")

advpropct = attainmentXSTEMct[5,] / apply(attainmentXSTEMct[4:5,], 2, sum) 
advpropct %>% kable(digits = 2, format = "pandoc", caption = "Advanced degree attainment, non-STEM and STEM degree holders (counts).")
```

And below is the effect on matching

```{r matcheffect, echo=FALSE, eval=TRUE, collapse=TRUE, results='asis'}

acs.all[acs.all$age >= 25 & acs.all$age <= 65,] %>% with(table(stemdeg, match)) %>% kable(digits = 0, row.names = T, format = "pandoc", caption = "STEM Degree X Match: all degree holders")

acs.all[acs.all$age >= 25 & acs.all$age <= 65 & acs.all$ed == 4,] %>% with(table(stemdeg, match)) %>% kable(digits = 0, row.names = T, format = "pandoc", caption = "STEM Degree X Match: excluding advanced degree holders")

allattaintab = acs.all[acs.all$age >= 25 & acs.all$age <= 65,] %>% with(tapply(perwt, list(stemdeg, match), sum)) 
baattaintab = acs.all[acs.all$age >= 25 & acs.all$age <= 65 & acs.all$ed == 4,] %>% with(tapply(perwt, list(stemdeg, match), sum))

allattaintab[2,2]/ apply(allattaintab, 1, sum)[2]

baattaintab[2,2] / apply(baattaintab, 1, sum)[2]
```

## Main effects

As a baseline, we will first estimate just the main effects for age, race, sex, and nativity.

```{r maineffects.mod, echo=FALSE, eval=TRUE, results='asis'}
# First look at differences in mean age by the other factors.
a %>% with(tapply(age, list(f_race, f_sex, f_fb), mean)) %>% ftable %>% round

# mod.1 = glm(match ~ stemjoblq + f_race + f_sex + f_fb + agec + I(agec^2) + log(stemjobcount), data = a, family = quasibinomial(link = "log"), start = rep(1, 10))
# if the above does not work...
mod.1 = glm(match ~ f_race + f_sex + f_fb + agec + I(agec^2), data = a, family = poisson)
# summary(mod.1)

cov.mod.1 = vcovHC(mod.1, type = "HC0")
std.err.mod.1 = sqrt(diag(cov.mod.1))

rmod1.est = data.frame(Estimate = coef(mod.1), "robust se" = std.err.mod.1, "Pr(>|z|)" = 2 * pnorm(abs(coef(mod.1)/std.err.mod.1), lower.tail = F),
 lcl = coef(mod.1) - (1.96 * std.err.mod.1),
 ucl = coef(mod.1) + (1.96 * std.err.mod.1))
# signif(rmod1.est, 3)

# To get the transformed standard errors
s = deltamethod(
    list(~exp(x1), ~exp(x2), ~exp(x3), ~exp(x4), ~exp(x5), ~exp(x6), ~exp(x7), ~exp(x8)), coef(mod.1), cov.mod.1)

# relative risks
rexp.est = exp(rmod1.est[,-3])
rexp.est[, "robust.se"] = s
# signif(rexp.est, 3)

kable(rexp.est, digits = 3, format = "pandoc")

# Below is code to just display the coefficients and standard errors
mod.me.coef = sprintf('%1.3f (%1.3f)', rexp.est[,1], rexp.est[,2])
# data.frame(names(coef(mod.1)), mod.me.coef)
thing = cbind(names(coef(mod.1)), mod.me.coef)

ll = round(as.numeric(logLik(mod.1)), 0)
bic = round(BIC.lm(mod.1), 0)

# kable(cbind(names(coef(mod.1)), mod.me.coef))
```


## Add LQ and STEM Job count

And from here on I will include both. 

```{r lq.mod, echo=FALSE, eval=TRUE, results='asis'}

mod.lq = glm(match ~ f_race + f_sex + f_fb + agec + I(agec^2) + stemjoblqc + stemjobcountc, data = a, family = poisson)
# summary(mod.lq)

cov.mod.lq = vcovHC(mod.lq, type = "HC0")
std.err.mod.lq = sqrt(diag(cov.mod.lq))

rmod1.est = data.frame(Estimate = coef(mod.lq), "robust se" = std.err.mod.lq, "Pr(>|z|)" = 2 * pnorm(abs(coef(mod.lq)/std.err.mod.lq), lower.tail = F),
 lcl = coef(mod.lq) - (1.96 * std.err.mod.lq),
 ucl = coef(mod.lq) + (1.96 * std.err.mod.lq))
# signif(rmod1.est, 3)

# To get the transformed standard errors
s = deltamethod(
    list(~exp(x1), ~exp(x2), ~exp(x3), ~exp(x4), ~exp(x5), ~exp(x6), ~exp(x7), ~exp(x8), ~exp(x9), ~exp(x10)), coef(mod.lq), cov.mod.lq)

# relative risks
rexp.est = exp(rmod1.est[,-3])
rexp.est[, "robust.se"] = s
# signif(rexp.est, 3)

kable(rexp.est, digits = 3)

# Below is an attempt to munge the coefficients/standard errors 
# for all the models into a single table. It failed.
mod.lq.coef = sprintf('%1.3f (%1.3f)', rexp.est[,1], rexp.est[,2])
# data.frame(names(coef(mod.lq)), mod.lq.coef)
# cbind(names(coef(mod.lq)), mod.lq.coef)

# kable(cbind(names(coef(mod.lq)), mod.lq.coef))

# mod.me.coef = c(mod.me.coef, NA) 
# kable(cbind(names(coef(mod.lq)), mod.me.coef, mod.lq.coef))
mod.lq.coef = sprintf('%1.3f (%1.3f)', rexp.est[,1], rexp.est[,2])
mod.lq.names = names(coef(mod.lq))

basemod = mod.lq
basemod.coef = mod.lq.coef
basemod.names = mod.lq.names

# kable(cbind(mod.lq.names, mod.lq.coef))
kable(cbind(basemod.names, basemod.coef))
```


 
## Now interact race and nativity

Given that the coefficient for foreign born is less than 1, I suspect that national origin might change that. Our baseline category is a white male, so it is conceivable that a white, foreign born male would have a lower probabilty of a match than a native born one. But I suspect the same is *not* true for, say, Asian males. 

Let's look.

```{r lq.mod.race, echo=FALSE, eval=TRUE, results='asis'}

mod.lq.race = glm(match ~ f_sex + f_race * f_fb + agec + I(agec^2) + stemjoblqc + stemjobcountc, data = a, family = poisson)
# summary(mod.lq.race)

cov.mod.lq.race = vcovHC(mod.lq.race, type = "HC0")
std.err.mod.lq.race = sqrt(diag(cov.mod.lq.race))

rmod1.est = data.frame(Estimate = coef(mod.lq.race), "robust se" = std.err.mod.lq.race, "Pr(>|z|)" = 2 * pnorm(abs(coef(mod.lq.race)/std.err.mod.lq.race), lower.tail = F),
 lcl = coef(mod.lq.race) - (1.96 * std.err.mod.lq.race),
 ucl = coef(mod.lq.race) + (1.96 * std.err.mod.lq.race))
# signif(rmod1.est, 3)

# To get the transformed standard errors
s = deltamethod(
    list(~exp(x1), ~exp(x2), ~exp(x3), ~exp(x4), ~exp(x5), ~exp(x6), ~exp(x7), ~exp(x8), ~exp(x9), ~exp(x10), ~exp(x11), ~exp(x12), ~exp(x13)), coef(mod.lq.race), cov.mod.lq.race)

# relative risks
rexp.est = exp(rmod1.est[,-3])
rexp.est[, "robust.se"] = s
# signif(rexp.est, 3)

kable(rexp.est, digits = 3)

mod.lqrace.coef = sprintf('%1.3f (%1.3f)', rexp.est[,1], rexp.est[,2])
mod.lqrace.names = names(coef(mod.lq.race))
kable(data.frame(mod.lqrace.names, mod.lqrace.coef), format = "pandoc")

# basemod = mod.lqrace
# basemod.coef = mod.lqrace.coef
# basemod.names = mod.lqrace.names
```

Look at that. The effect for Asian alone is no longer statistically significant, but the interaction with Asian and Foreign Born does all the work that Asian did *without* the interaction. In other words, the matching effect for Asian men is *only* for foreign-born Asian men. Also, as expected, the effect is negative for foreign born Hispanic men, and unsurprisingly not statistically significant for foreign born blacks (given the relatively small number).

# Interaction between demography and geography

In the following sections we will interact each demographic variable (race, sex, nativity) with the LQ variable to see whether place attenuates or exaggerates the demographic effects.

In each of the following sections we will interact race, sex, and nativity with the geographic variables. We expect better matching probabilities in high LQ locations.

In this section I have maintained the interaction between race and nativity. That means we have three way interactions between race/nativity/LQ.

Since it does more work, I only interact with the stemjob LQ variable. To be thorough, we should probably do both... but then the number of coefficients gets pretty crazy.

## Interact sex

What is the effect of geography for women?

Here I have removed the interaction between race and nativity for the race/sex/nativity * lq models.

```{r sex, echo=FALSE, eval=TRUE, results='asis'}
mod.1a = glm(match ~ stemjoblqc * f_sex + f_race + f_fb + agec + I(agec^2) + stemjobcountc, data = a, family = poisson) 
# basic goodness of fit for model
with(mod.1a, cbind(deviance, df.residual, P = pchisq(deviance, df.residual, lower.tail = F)))

cov.mod.1a = vcovHC(mod.1a, type = "HC0")
std.err.mod.1a = sqrt(diag(cov.mod.1a))

rmod1.est = data.frame(Estimate = coef(mod.1a), "robust se" = std.err.mod.1a, "Pr(>|z|)" = 2 * pnorm(abs(coef(mod.1a)/std.err.mod.1a), lower.tail = F),
 lcl = coef(mod.1a) - (1.96 * std.err.mod.1a),
 ucl = coef(mod.1a) + (1.96 * std.err.mod.1a))
# signif(rmod1.est, 3)

# To get the transformed standard errors
s = deltamethod(
    list(~exp(x1), ~exp(x2), ~exp(x3), ~exp(x4), ~exp(x5), ~exp(x6), ~exp(x7), ~exp(x8), ~exp(x9), ~exp(x10), ~exp(x11)), coef(mod.1a), cov.mod.1a)

# relative risks
rexp.est = exp(rmod1.est[,-3])
rexp.est[, "robust.se"] = s
# signif(rexp.est, 3)

kable(rexp.est, digits = 3)

mod.isex.coef = sprintf('%1.3f (%1.3f)', rexp.est[,1], rexp.est[,2])
mod.isex.names = names(coef(mod.1a))
# mod.isex.coef %>% data.frame %>% kable(format = "pandoc")

kable(cbind(mod.isex.names, mod.isex.coef), format = "pandoc")

```

Being in a high STEM job LQ city has a small but positive effect on the probability of a match for a woman, increasing anywhere between 2% and 10%.

This effect might be a result of a lower relative number of women in STEM job concentrations.

```{r sextab, echo=FALSE, eval=TRUE, results='hide', cache=TRUE}
sextab = a %>% with(tapply(perwt, list(geog_curr, f_sex), sum))
fprop = sextab[,"F"] / apply(sextab, 1, sum)

lqs = a %>% with(tapply(stemjoblq, list(geog_curr), mean))
data.frame(citynames, lqs, fprop) %>% kable(digits = 2, format = "pandoc", caption = "Proportion of women")

plot(fprop ~ lqs, main = "Proportion of STEM degree holders who are women, by LQ", bty = "n")
abline(lm(fprop ~ lqs))
summary(lm(fprop ~ lqs))

quickscatter(predictor = lqs, outcome = fprop, line = "straight")
quickscatter(predictor = lqs, outcome = fprop, line = "lowess")
```

Except that there is not much variation in the proportion of STEM degree holders who are women. What about proportion matched?

```{r sextab2, echo=FALSE, eval=TRUE, results='asis'}
sextab = a[a$match == 1, ] %>% with(tapply(perwt, list(geog_curr, f_sex), sum))
fpropmatch = sextab[,"F"] / apply(sextab, 1, sum)

lqs = a %>% with(tapply(stemjoblq, list(geog_curr), mean))
data.frame(citynames, lqs, fprop, fpropmatch, fprop - fpropmatch) %>% kable(digits = 2, format = "pandoc", caption = "Proportion of women")

plot(fpropmatch ~ lqs, main = "Proportion of the matched who are women, by LQ", bty = "n")
abline(lm(fpropmatch ~ lqs))
summary(lm(fpropmatch ~ lqs))

cor(fpropmatch, lqs)
```

So, there is a mild association (higher LQ corresponds to a higher proportion of women who are matched), but the effect is weak.

## Interact nativity

```{r nativity, echo=FALSE, eval=TRUE, results='asis'}
mod.1b = glm(match ~ stemjobcountc + stemjoblqc + f_sex + f_race + f_fb + agec + I(agec^2)  + stemjoblqc:f_fb, data = a, family = poisson) 
with(mod.1b, cbind(deviance, df.residual, P = pchisq(deviance, df.residual, lower.tail = F)))

cov.mod.1b = vcovHC(mod.1b, type = "HC0")
std.err.mod.1b = sqrt(diag(cov.mod.1b))

rmod1.est = data.frame(Estimate = coef(mod.1b), "robust se" = std.err.mod.1b, "Pr(>|z|)" = 2 * pnorm(abs(coef(mod.1b)/std.err.mod.1b), lower.tail = F),
 lcl = coef(mod.1b) - (1.96 * std.err.mod.1b),
 ucl = coef(mod.1b) + (1.96 * std.err.mod.1b))
# signif(rmod1.est, 3)

# To get the transformed standard errors
s = deltamethod(
    list(~exp(x1), ~exp(x2), ~exp(x3), ~exp(x4), ~exp(x5), ~exp(x6), ~exp(x7), ~exp(x8), ~exp(x9), ~exp(x10), ~exp(x11)), coef(mod.1b), cov.mod.1b)

# relative risks
rexp.est = exp(rmod1.est[,-3])
rexp.est[, "robust.se"] = s
# signif(rexp.est, 3)

kable(rexp.est, digits = 3)

mod.inat.coef = sprintf('%1.3f (%1.3f)', rexp.est[,1], rexp.est[,2])
mod.inat.names = names(coef(mod.1b))
kable(cbind(mod.inat.names, mod.inat.coef))

```

Note the direction change from the previous version of this file I sent. Calculating the LQ of STEM jobs correctly, now has made the effect modification for the foreign born *negative*. Being in a higher LQ city **lowers** (slightly) the effect modification for a match.

This is hard to believe. 

Although, an anecdote might inform. Kristi Copeland's husband is from the subcontinent, and his first job was in Cleveland with Booz, Allen, and Hamilton. There were *lots* of foreign born in that office and they all joked that they accepted the job in Cleveland because they didn't know better. 

## Interact race

```{r race, echo=FALSE, eval=TRUE, results='asis'}
mod.1c = glm(match ~ stemjoblqc + stemjobcountc + f_sex + f_fb + f_race*stemjoblqc + agec + I(agec^2), data = a, family = poisson) 
with(mod.1c, cbind(deviance, df.residual, P = pchisq(deviance, df.residual, lower.tail = F)))

cov.mod.1c = vcovHC(mod.1c, type = "HC0")
std.err.mod.1c = sqrt(diag(cov.mod.1c))

rmod1.est = data.frame(Estimate = coef(mod.1c), "robust se" = std.err.mod.1c, "Pr(>|z|)" = 2 * pnorm(abs(coef(mod.1c)/std.err.mod.1c), lower.tail = F),
 lcl = coef(mod.1c) - (1.96 * std.err.mod.1c),
 ucl = coef(mod.1c) + (1.96 * std.err.mod.1c))
# signif(rmod1.est, 3)

# To get the transformed standard errors
s = deltamethod(
    list(~exp(x1), ~exp(x2), ~exp(x3), ~exp(x4), ~exp(x5), ~exp(x6), ~exp(x7), ~exp(x8), ~exp(x9), ~exp(x10), ~exp(x11), ~exp(x12), ~exp(x13)), coef(mod.1c), cov.mod.1c)

# relative risks
rexp.est = exp(rmod1.est[,-3])
rexp.est[, "robust.se"] = s
# signif(rexp.est, 3)

kable(rexp.est, digits = 3)

mod.irace.coef = sprintf('%1.3f (%1.3f)', rexp.est[,1], rexp.est[,2])
mod.irace.names = names(coef(mod.1c))
# kable(cbind(mod.irace.names, mod.irace.coef))

```

Perhaps a little surprisingly, Asian men, are a little *less* likely to be matched in high STEM job LQ cities than white men. For Hispanic men it is marginally positive, but not well estimated enough to be definitive, and for Black men, strongly positive.

So, now we need to put all of this together.

# Model-based assessment of interactions

I think the more standard way is to put all interactions into a single model, then test the differences between models, pulling one out at a time.

But, here I will just present a table of each model estimated separately. There are many roads to Rome...

```{r mod.interactions, echo=FALSE, eval=TRUE, results='asis'}
mod.all = glm(match ~ stemjoblqc + stemjobcountc + f_sex + f_race + f_fb + agec + I(agec^2) +
              stemjoblqc:f_sex + 
              stemjoblqc:f_fb +
              stemjoblqc:f_race,
              data = a, family = poisson) 

cov.mod.all = vcovHC(mod.all, type = "HC0")
std.err.mod.all = sqrt(diag(cov.mod.all))

rmod1.est = data.frame(Estimate = coef(mod.all), "robust se" = std.err.mod.all, "Pr(>|z|)" = 2 * pnorm(abs(coef(mod.all)/std.err.mod.all), lower.tail = F),
 lcl = coef(mod.all) - (1.96 * std.err.mod.all),
 ucl = coef(mod.all) + (1.96 * std.err.mod.all))
# signif(rmod1.est, 3)

# To get the transformed standard errors
s = deltamethod(
    list(~exp(x1), ~exp(x2), ~exp(x3), ~exp(x4), ~exp(x5), ~exp(x6), ~exp(x7), ~exp(x8), ~exp(x9), ~exp(x10), ~exp(x11), ~exp(x12), ~exp(x13), ~exp(x14), ~exp(x15)), coef(mod.all), cov.mod.all)

# relative risks
rexp.est = exp(rmod1.est[,-3])
rexp.est[, "robust.se"] = s
# signif(rexp.est, 3)

kable(rexp.est, digits = 3)

mod.iall.coef = sprintf('%1.3f (%1.3f)', rexp.est[,1], rexp.est[,2])
mod.iall.names = names(coef(mod.all))
# kable(cbind(mod.iall.names, mod.iall.coef))

```

Now figure out a way to display all of that together....

```{r putitalltogether, echo=TRUE, eval=TRUE, collapse=TRUE, results='asis'}

kable(data.frame(basemod.names, basemod.coef), format = "pandoc")
kable(data.frame(mod.isex.names, mod.isex.coef), format = "pandoc")
kable(data.frame(mod.inat.names, mod.inat.coef), format = "pandoc")
kable(data.frame(mod.irace.names, mod.irace.coef), format = "pandoc")
kable(cbind(mod.iall.names, mod.iall.coef), format = "pandoc")

# make a data frame with the mod.iall names, and coefs as the 4th column
length(mod.iall.coef)

res = data.frame(matrix(data = NA, nrow = length(mod.iall.coef), ncol = 5), row.names = mod.iall.names)

idx.base = match(basemod.names, mod.iall.names)
idx.sex = match(mod.isex.names, mod.iall.names)
idx.nat = match(mod.inat.names, mod.iall.names)
idx.race = match(mod.irace.names, mod.iall.names)

res[idx.base, 1] = basemod.coef
res[idx.sex, 2] = mod.isex.coef
res[idx.nat, 3] = mod.inat.coef
res[idx.race, 4] = mod.irace.coef
res[,5] = mod.iall.coef

names(res) = c("base", "lq:sex", "lq:nativity", "lq:race", "full")
kable(res, digits = 3, format = "pandoc")
```


## Model Comparison 

Now remove factors and compare the models.

```{r mod.interactions.test, echo=FALSE, eval=TRUE, results='markup'}
# the gender interaction
anova(basemod, mod.1a, test = "Chisq")

ll.1a= -2 * (logLik(basemod) - logLik(mod.1a))[1]
df.1a = summary(mod.1a)$df[1] - summary(basemod)$df[1]
pchisq(ll.1a, df = df.1a, lower.tail = F)
ll.1a = pchisq(ll.1a, df = df.1a, lower.tail = F)

# the natitivy interaction
anova(basemod, mod.1b, test = "Chisq")

ll.1b= -2 * (logLik(basemod) - logLik(mod.1b))[1]
df.1b = summary(mod.1b)$df[1] - summary(basemod)$df[1]
pchisq(ll.1b, df = df.1b, lower.tail = F)
ll.1b = pchisq(ll.1b, df = df.1b, lower.tail = F)

# and the race interaction
anova(basemod, mod.1c, test = "Chisq")

ll.1c = -2 * (logLik(basemod) - logLik(mod.1c))[1]
df.1c = summary(mod.1c)$df[1] - summary(basemod)$df[1]
pchisq(ll.1c, df = df.1c, lower.tail = F)
ll.1c = pchisq(ll.1c, df = df.1c, lower.tail = F)

# and the kitchen sink
anova(basemod, mod.all, test = "Chisq")

ll.all = -2 * (logLik(basemod) - logLik(mod.all))[1]
df.all = summary(mod.all)$df[1] - summary(basemod)$df[1]
pchisq(ll.all, df = df.all, lower.tail = F)
ll.all = pchisq(ll.all, df = df.all, lower.tail = F)

lr.Chisq = c(NA, ll.1a, ll.1b, ll.1c, ll.all)
# Information criteria
lls = lapply(list(basemod, mod.1a, mod.1b, mod.1c, mod.all), function(x) {logLik(x)[1]}) %>% as.data.frame
names(lls) = c("base", "sex", "nativity", "race", "all")

BICs = lapply(list(basemod, mod.1a, mod.1b, mod.1c, mod.all), BIC.lm) %>% as.data.frame
names(BICs) = c("base", "sex", "nativity", "race", "all")

kable(rbind(lr.Chisq, lls, BICs), digits = 3, format = "pandoc")
crit = rbind(lr.Chisq, lls, BICs) %>% round(3) %>% data.frame
rownames(crit) = c("Pr[>Chi]", "Log Likelihood", "BIC")
dim(crit) ; dim(res)
colnames(crit) = names(res)
kable(rbind(res, crit), digits = 3, format = "pandoc")
```

So, again, results are mixed. Looking at the Wald tests (in the previous section) each interaction term has some explanatory power --or, rather some subsets of each do. But the overall model fit is *worse* with all the interactions in. And the best fitting model overall is the one with no interactions.  

I suspect that with our reduced number of observations (now down to ~ 93,000) with some really small cell sizes (e.g. foreign born black women with a STEM degree) the BIC is telling us that we do not have enough data to make a strong case for the effects we see with the coefficients. 

# Fully saturated model

Operation explosion:

```{r explode, echo=FALSE, eval=FALSE, results='asis'}
# subset down to everything but San Jose
mod.explode = glm(match ~ stemjobcountc + agec + I(agec^2) +
              stemjoblqc*f_sex*f_fb*f_race,
              data = a, family = poisson) 

kable(anova(mod.explode), format = "pandoc", digits = 2)
```

As an exploratory exercise, this is interesting. Sex does three times the work than each of the three next largest variables: stemjoblq, race, and age. Next is the interaction between nativity and race which tells us that nativity is effectively meaningless in the model unless it is considered together with race. Sex also has a pretty big modification effect on nativity. The geography effect is big by itself, but has very, very little influence on the other factors, except race. 

```{r explode.coefs, echo=FALSE, eval=FALSE, results='markup'}
summary(mod.explode)
```

The race effect is small and weak, and appears to be marginally positive for Blacks, while immesurably small for Asians and Hispanics. 

Our conclusions are mostly unchanged. Except to say that I was right in the way I specified the model. 

```{r explode.se, echo=FALSE, eval=FALSE, results='markup'}
# cov.mod.explode = vcovHC(mod.explode, type = "HC0")
std.err.mod.explode = sqrt(diag(cov.mod.explode))

rmod1.est = data.frame(Estimate = coef(mod.explode), "robust se" = std.err.mod.explode, "Pr(>|z|)" = 2 * pnorm(abs(coef(mod.explode)/std.err.mod.explode), lower.tail = F),
 lcl = coef(mod.explode) - (1.96 * std.err.mod.explode),
 ucl = coef(mod.explode) + (1.96 * std.err.mod.explode))
# signif(rmod1.est, 3)

# To get the transformed standard errors
s = deltamethod(
    list(~exp(x1), ~exp(x2), ~exp(x3), ~exp(x4), ~exp(x5), ~exp(x6), ~exp(x7), ~exp(x8), ~exp(x9), ~exp(x10), ~exp(x11), ~exp(x12), ~exp(x13), ~exp(x14), ~exp(x15), ~exp(x16), ~exp(x17), ~exp(x18), ~exp(x19), ~exp(x20), ~exp(x21), ~exp(x22), ~exp(x23), ~exp(x24), ~exp(x25), ~exp(x26), ~exp(x27), ~exp(x28), ~exp(x29), ~exp(x30), ~exp(x31), ~exp(x32), ~exp(x33), ~exp(x34), ~exp(x35)), coef(mod.explode), cov.mod.explode)

# relative risks
rexp.est = exp(rmod1.est[,-3])
rexp.est[, "robust.se"] = s
# signif(rexp.est, 3)

kable(rexp.est, digits = 3)

mod.iexplode.coef = sprintf('%1.3f (%1.3f)', rexp.est[,1], rexp.est[,2])
mod.iexplode.names = names(coef(mod.explode))
BIC.lm(mod.all) ; BIC.lm(mod.explode)

m = match(mod.iall.names, mod.iexplode.names)

kable(cbind(mod.iexplode.names, mod.iexplode.coef, mod.iall.coef))

```

## ROC!

```{r rocit, echo=TRUE, eval=TRUE, collapse=TRUE, results='markup'}
source('~/Documents/rfuns/roclines.R')
# Get some good colors
require(RColorBrewer)
display.brewer.all()
pal = brewer.pal(n = 11, "PRGn")

roc.all = roclines(mod.all, a$match)
roc.explode = roclines(mod.explode, a$match)

png(file = "../results/figs/roc1.png", height = 640, width = 640)

greygrid2(x = roc.all[,"fpr"], y = roc.all[,"tpr"])
points(x = roc.all[,"fpr"], y = roc.all[,"tpr"], type = "s", lwd = 1.5, col = pal[1])
points(x = roc.explode[,"fpr"], y = roc.explode[,"tpr"], type = "s", lwd = 1.5, col = pal[11])
abline(c(0,1), lty = 2, col = "grey50")
title(main = "Predictive difference (ROC Curves)", xlab = "False Positive Rate", ylab = "True Positive Rate")
legend("topleft", legend = c("Full Model", "Saturated Model"), lty = 1, lwd = 1.5, col = c(pal[1], pal[11]), bty = "o", cex = 0.85, box.col = "gray50")

dev.off()
```

